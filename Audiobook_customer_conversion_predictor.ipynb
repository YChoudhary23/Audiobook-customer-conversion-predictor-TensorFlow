{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6137c2c4",
   "metadata": {},
   "source": [
    "### This notebook contains a Classification model using TensorFlow to predict Audiobook customer conversion\n",
    "*  **Data source**: Audiobooks_data.csv (Available in github repo) <br>\n",
    "*  **Data preprocessing**: Shuffling dataset, Balancing dataset,Scaling inputs,Splitting into training,validattion & test, Saving into .npz format <br>\n",
    "*  **Model building**: Loading the three .npz files, Training the model <br>\n",
    "*  **Model Evaluation**: Evaluating the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c4c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71ff5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14084, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = np.loadtxt('E:\\\\Udemy\\\\Data science\\\\Python\\\\Deep Learning\\\\Classification\\\\Business case\\\\Audiobooks_data.csv',delimiter=',')\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017320b",
   "metadata": {},
   "source": [
    "#### Shuffling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b049ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_indices = np.arange(raw_data.shape[0])\n",
    "np.random.shuffle(shuffle_indices)\n",
    "shuffled_data = raw_data[shuffle_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87aa706",
   "metadata": {},
   "source": [
    "#### Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988646dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "input_data = shuffled_data[:,1:-1]\n",
    "target_data = shuffled_data[:,-1]\n",
    "\n",
    "num_ones = int(np.sum(target_data))\n",
    "num_zeroes = 0\n",
    "tobe_removed =[]\n",
    "for i in range(target_data.shape[0]):\n",
    "    if(target_data[i] == 0):\n",
    "        num_zeroes +=1\n",
    "        if(num_zeroes > num_ones):\n",
    "            tobe_removed.append(i)\n",
    "\n",
    "balanced_input = np.delete(input_data, tobe_removed, axis=0)\n",
    "balanced_output = np.delete(target_data, tobe_removed, axis=0)\n",
    "print(int(np.sum(balanced_output)) / balanced_output.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9d98a",
   "metadata": {},
   "source": [
    "#### Scaling the Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "897b747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(balanced_input)\n",
    "scaled_input = scaler.transform(balanced_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89e69c",
   "metadata": {},
   "source": [
    "#### Splitting dataset into training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56fc91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5024838549428713\n",
      "0.47767857142857145\n",
      "0.5422885572139303\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_train, input_test, output_train, output_test = train_test_split(scaled_input, balanced_output, test_size=0.1, random_state=42)\n",
    "validation_size = int(0.1 * input_train.shape[0])\n",
    "validation_input =  input_train[input_train.shape[0]-validation_size:]\n",
    "validation_output = output_train[output_train.shape[0]-validation_size:]\n",
    "#Confirming balance of each dataset\n",
    "print(np.sum(output_train)/output_train.shape[0])\n",
    "print(np.sum(output_test)/output_test.shape[0])\n",
    "print(np.sum(validation_output)/validation_output.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e0fcb4",
   "metadata": {},
   "source": [
    "#### Saving the three datasets in .npz format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "479e6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Training_audiobook.npz', features= input_train, targets= output_train)\n",
    "np.savez('Validation_audiobook.npz', features= input_test, targets= output_test)\n",
    "np.savez('Test_audiobook.npz', features= validation_input, targets= validation_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd66015",
   "metadata": {},
   "source": [
    "#### Loading .npz datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a524709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.load('Training_audiobook.npz')\n",
    "#print(temp['targets'][0])\n",
    "train_inputs, train_outputs = temp['features'].astype(float), temp['targets'].astype(int)\n",
    "#print(train_outputs[0])\n",
    "temp = np.load('Validation_audiobook.npz')\n",
    "val_inputs, val_outputs = temp['features'].astype(float), temp['targets'].astype(int)\n",
    "temp = np.load('Test_audiobook.npz')\n",
    "test_inputs, test_outputs = temp['features'].astype(float), temp['targets'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1e3f9",
   "metadata": {},
   "source": [
    "#### Classification model using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff56917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa2a47dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "41/41 - 1s - loss: 0.5573 - accuracy: 0.7181 - val_loss: 0.4977 - val_accuracy: 0.7165 - 997ms/epoch - 24ms/step\n",
      "Epoch 2/100\n",
      "41/41 - 0s - loss: 0.4353 - accuracy: 0.7839 - val_loss: 0.4382 - val_accuracy: 0.7589 - 171ms/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12e5c852bc8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "output_size = 2\n",
    "hidden_width = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Dense(hidden_width, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_width, activation='relu'),\n",
    "                            #tf.keras.layers.Dense(hidden_width, activation='sigmoid'),\n",
    "                            #tf.keras.layers.Dense(hidden_width, activation='sigmoid'),\n",
    "                            #tf.keras.layers.Dense(hidden_width, activation='sigmoid'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "max_epochs = 100\n",
    "early_stop = tf.keras.callbacks.EarlyStopping()\n",
    "model.fit(train_inputs,\n",
    "          train_outputs,\n",
    "          batch_size = batch_size,\n",
    "          epochs = max_epochs,\n",
    "          callbacks = [early_stop],\n",
    "          validation_data = (val_inputs,val_outputs),\n",
    "          verbose=2\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c55663",
   "metadata": {},
   "source": [
    "#### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59b23904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 6ms/step - loss: 0.4220 - accuracy: 0.7587\n",
      "Test Loss: 0.42 Test Accuracy: 75.87%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs,test_outputs)\n",
    "print('Test Loss: {:.2f} Test Accuracy: {:.2f}%'.format(test_loss, test_accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
